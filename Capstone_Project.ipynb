{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The capstone project aims to answer questions about the immigration to U.S.. Questions about the immigrants profile, the most visa types issued by U.S. government, weather patterns and how they can influence the immigration's statistics are some of the answers we can get from the data provided for this project.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The project's goal is to use 3 datasets and manipulate them using Pyspark. By doing so, it will be created a data model proposed by Ralph Kimball called star schema. It will also be created 6 dimensional tables and 1 fact table.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "For this project it was used 3 data sources:\n",
    "- **I94 immigration data**: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://www.trade.gov/national-travel-and-tourism-office) is where the data comes from.\n",
    "- **World Temperature Data**: The dataset comes from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "- **U.S. City Demographic Data**: This data comes from OpenSoft. You can read more about it [here](https://datahub.io/core/airport-codes#data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Exploring the Data and Cleaning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum       |fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|6.0  |2016.0|4.0   |692.0 |692.0 |XXX    |20573.0|null   |null   |null   |37.0  |2.0    |1.0  |null    |null    |null |T      |null   |U      |null   |1979.0 |10282016|null  |null  |null   |1.897628485E9|null |B2      |\n",
      "|7.0  |2016.0|4.0   |254.0 |276.0 |ATL    |20551.0|1.0    |AL     |null   |25.0  |3.0    |1.0  |20130811|SEO     |null |G      |null   |Y      |null   |1991.0 |D/S     |M     |null  |null   |3.73679633E9 |00296|F1      |\n",
      "|15.0 |2016.0|4.0   |101.0 |101.0 |WAS    |20545.0|1.0    |MI     |20691.0|55.0  |2.0    |1.0  |20160401|null    |null |T      |O      |null   |M      |1961.0 |09302016|M     |null  |OS     |6.66643185E8 |93   |B2      |\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert SAS date into Pyspark date.\n",
    "convert_date = F.udf(lambda x: (datetime(1960, 1, 1).date() + timedelta(x)).isoformat() if x else None)\n",
    "\n",
    "df = df.withColumn('arrival_date', convert_date('arrdate')).withColumn('departure_date', convert_date('depdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('entdepa', 'entdepu', 'entdepd', 'insnum', 'admnum', 'fltno', 'occup', 'visapost', 'dtadfile', 'count', 'dtaddto', 'i94yr', 'matflag', 'arrdate', 'depdate', 'airline', 'i94mode', 'i94visa', 'i94mon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-------+-------+------+-------+------+--------+------------+--------------+\n",
      "|cicid|i94cit|i94res|i94port|i94addr|i94bir|biryear|gender|visatype|arrival_date|departure_date|\n",
      "+-----+------+------+-------+-------+------+-------+------+--------+------------+--------------+\n",
      "|6.0  |692.0 |692.0 |XXX    |null   |37.0  |1979.0 |null  |B2      |2016-04-29  |null          |\n",
      "|7.0  |254.0 |276.0 |ATL    |AL     |25.0  |1991.0 |M     |F1      |2016-04-07  |null          |\n",
      "|15.0 |101.0 |101.0 |WAS    |MI     |55.0  |1961.0 |M     |B2      |2016-04-01  |2016-08-25    |\n",
      "+-----+------+------+-------+-------+------+-------+------+--------+------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('visatype', 'visa_type')\\\n",
    "        .withColumnRenamed('cicid', 'id')\\\n",
    "        .withColumnRenamed('i94cit', 'citizenship_code')\\\n",
    "        .withColumnRenamed('i94res', 'residence_code')\\\n",
    "        .withColumnRenamed('i94port', 'city_code')\\\n",
    "        .withColumnRenamed('i94addr', 'state_code')\\\n",
    "        .withColumnRenamed('i94bir', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------+---------+----------+----+-------+------+---------+------------+--------------+\n",
      "|id       |citizenship_code|residence_code|city_code|state_code|age |biryear|gender|visa_type|arrival_date|departure_date|\n",
      "+---------+----------------+--------------+---------+----------+----+-------+------+---------+------------+--------------+\n",
      "|6073731.0|123.0           |123.0         |PHU      |IL        |50.0|1966.0 |M     |WB       |2016-04-20  |2001-07-20    |\n",
      "|5930761.0|112.0           |112.0         |MIA      |CA        |79.0|1937.0 |F     |WT       |2016-04-11  |2012-04-12    |\n",
      "|5932223.0|999.0           |112.0         |SDP      |CA        |56.0|1960.0 |F     |WT       |2016-04-11  |2012-04-12    |\n",
      "|5904464.0|180.0           |135.0         |HHW      |HI        |75.0|1941.0 |F     |WT       |2016-04-05  |2012-04-14    |\n",
      "|5930054.0|438.0           |464.0         |HHW      |HI        |73.0|1943.0 |M     |WT       |2016-04-20  |2014-04-22    |\n",
      "|5944554.0|126.0           |126.0         |LAN      |IN        |78.0|1938.0 |F     |B2       |2016-04-30  |2014-05-17    |\n",
      "|5513937.0|339.0           |339.0         |PEM      |MO        |15.0|2001.0 |M     |B2       |2016-04-06  |2015-04-18    |\n",
      "|6020095.0|273.0           |135.0         |CHI      |CA        |67.0|1949.0 |M     |B2       |2016-04-30  |2015-05-15    |\n",
      "|1385508.0|135.0           |526.0         |MIA      |FL        |37.0|1979.0 |M     |WT       |2016-04-08  |2015-05-18    |\n",
      "|5918782.0|438.0           |438.0         |HHW      |ID        |78.0|1938.0 |M     |WT       |2016-04-23  |2015-05-30    |\n",
      "+---------+----------------+--------------+---------+----------+----+-------+------+---------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('departure_date').where('departure_date < arrival_date').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------------+---------+----------+----+-------+------+---------+------------+--------------+\n",
      "|id  |citizenship_code|residence_code|city_code|state_code|age |biryear|gender|visa_type|arrival_date|departure_date|\n",
      "+----+----------------+--------------+---------+----------+----+-------+------+---------+------------+--------------+\n",
      "|15.0|101.0           |101.0         |WAS      |MI        |55.0|1961.0 |M     |B2       |2016-04-01  |2016-08-25    |\n",
      "|27.0|101.0           |101.0         |BOS      |MA        |58.0|1958.0 |M     |B1       |2016-04-01  |2016-04-05    |\n",
      "|28.0|101.0           |101.0         |ATL      |MA        |56.0|1960.0 |F     |B1       |2016-04-01  |2016-04-05    |\n",
      "|29.0|101.0           |101.0         |ATL      |MA        |62.0|1954.0 |M     |B2       |2016-04-01  |2016-04-17    |\n",
      "|30.0|101.0           |101.0         |ATL      |NJ        |49.0|1967.0 |M     |B2       |2016-04-01  |2016-05-04    |\n",
      "|31.0|101.0           |101.0         |ATL      |NY        |43.0|1973.0 |M     |B2       |2016-04-01  |2016-06-06    |\n",
      "|33.0|101.0           |101.0         |HOU      |TX        |53.0|1963.0 |F     |B2       |2016-04-01  |2016-04-10    |\n",
      "|36.0|101.0           |101.0         |NYC      |NJ        |37.0|1979.0 |M     |B2       |2016-04-01  |2016-04-17    |\n",
      "|37.0|101.0           |101.0         |NYC      |NJ        |49.0|1967.0 |F     |B2       |2016-04-01  |2016-04-23    |\n",
      "|38.0|101.0           |101.0         |NYC      |NY        |33.0|1983.0 |M     |B2       |2016-04-01  |2016-05-01    |\n",
      "+----+----------------+--------------+---------+----------+----+-------+------+---------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We checked out there are rows with departure date being earlier than arrival date, which should be impossible. Let's clean this.\n",
    "df = df.where('departure_date > arrival_date')\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the port_codes.\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "city_regex = re.compile(r\"(\\s*)\\'(.{0,3})\\'[\\t*\\s*]=[\\t*\\s*]\\'(.*)\\'\")\n",
    "#city_regex = re.compile(r\"(\\s*)\\'([A-Z]{3})\\'\\t=\\t\\'([^\\s^\\t]*$)\")\n",
    "port_codes = {}\n",
    "for city in data[303:893]:\n",
    "    founds = city_regex.search(city)\n",
    "    port_codes[founds.group(2)] = founds.group(3)\n",
    "\n",
    "list_map = list(map(list, port_codes.items()))\n",
    "\n",
    "port_codes_df = spark.createDataFrame(list_map, ['port_code', 'location'])\n",
    "\n",
    "port_codes_df = port_codes_df.withColumn('location', F.trim(F.col('location')))\n",
    "port_codes_df = port_codes_df.withColumn('city_name', F.split('location', ',')[0])\\\n",
    "        .withColumn('state_code', F.trim(F.split('location', ',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+------------------------+----------+\n",
      "|port_code|location                    |city_name               |state_code|\n",
      "+---------+----------------------------+------------------------+----------+\n",
      "|ANC      |ANCHORAGE, AK               |ANCHORAGE               |AK        |\n",
      "|BAR      |BAKER AAF - BAKER ISLAND, AK|BAKER AAF - BAKER ISLAND|AK        |\n",
      "|DAC      |DALTONS CACHE, AK           |DALTONS CACHE           |AK        |\n",
      "+---------+----------------------------+------------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_codes_df.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "As previously mentioned, the chosen data model was the star schema, which was proposed by Ralph Kimball. That model was the chosen one because it allows great performance, which is desired since the dataset can increase, and it also allows users to write simple queries joining the fact and dimension tables in order to achieve the analytical dataset they need. The tables are as follow:\n",
    "![Data Model1](assets/data_model2.drawio.svg)\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Create a Spark dataframe with SAS data and csv files.\n",
    "2. Clean the data by removing duplicates, NA values, and renaming fields with more meaningful name.\n",
    "3. Create dimension tables and fact tables.\n",
    "4. Save the final tables on parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating dim_time_table\n",
    "'arrival_date'\n",
    "def create_dim_time_table(spark_df):\n",
    "    \"\"\"\n",
    "        It takes dates from a spark_df and extracts its month, day, week, year.\n",
    "        \n",
    "        :param: spark_df: spark dataframe\n",
    "    \"\"\"\n",
    "      \n",
    "    spark_df = spark_df.select(F.explode(F.array('arrival_date', 'departure_date')).alias('date'))    \n",
    "    \n",
    "    \n",
    "    # Make it wider with more info\n",
    "    spark_df = spark_df.withColumn('day', F.dayofmonth(spark_df['date']))\\\n",
    "    .withColumn('month', F.month(spark_df['date']))\\\n",
    "    .withColumn('year', F.year(spark_df['date']))\\\n",
    "    .withColumn('week', F.weekofyear(spark_df['date']))\n",
    "    \n",
    "    spark_df = spark_df.drop_duplicates(subset=['date'])  \n",
    "    \n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+----+----+\n",
      "|date      |day|month|year|week|\n",
      "+----------+---+-----+----+----+\n",
      "|2016-04-01|1  |4    |2016|13  |\n",
      "|2016-04-02|2  |4    |2016|13  |\n",
      "|2016-04-03|3  |4    |2016|13  |\n",
      "+----------+---+-----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_time_table = create_dim_time_table(df)\n",
    "dim_time_table.orderBy('date').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_immigrant_table(spark_df):\n",
    "    \"\"\"\n",
    "        It takes a spark dataframe and extract immigrant data.\n",
    "        \n",
    "        :param: spark_df: spark dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark_df.select('id', 'age', 'biryear', 'gender')\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_immigrant = create_dim_immigrant_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+------+\n",
      "|id   |age |biryear|gender|\n",
      "+-----+----+-------+------+\n",
      "|558.0|42.0|1974.0 |M     |\n",
      "|596.0|24.0|1992.0 |M     |\n",
      "|934.0|54.0|1962.0 |F     |\n",
      "+-----+----+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrant.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# removing fields that belong to the dimension table.\n",
    "df = df.drop('age', 'biryear', 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating dim_visa_type table\n",
    "def create_dim_visa_type(spark_df):\n",
    "    \"\"\"\n",
    "        It takes spark_df and creates a visa table.\n",
    "        \n",
    "        :param: spark_df: spark dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the unique visa types\n",
    "    df = spark_df.select('visa_type').distinct()\n",
    "    \n",
    "    df = df.withColumn('visa_id', F.monotonically_increasing_id())\n",
    "    \n",
    "    df.createOrReplaceTempView('visa_code')\n",
    "    \n",
    "    # Classifying visa tyes into 3 categories, according to the data source\n",
    "    df = spark.sql(\"\"\"\n",
    "                SELECT visa_id, visa_type, \n",
    "                CASE \n",
    "                    WHEN visa_type in ('B1', 'WB', 'GB', 'GMB', 'I', 'I1', 'E1', 'E2') THEN 'Business'\n",
    "                    WHEN visa_type in ('B2', 'WT', 'GT', 'GMT', 'CP', 'CPL', 'SBP') THEN 'Pleasure'\n",
    "                    WHEN visa_type in ('F1', 'F2', 'M1', 'M2') THEN 'Student'\n",
    "                    ELSE 'not defined'\n",
    "                END as visa_category\n",
    "                FROM visa_code\n",
    "            \"\"\")\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------+\n",
      "|visa_id      |visa_type|visa_category|\n",
      "+-------------+---------+-------------+\n",
      "|103079215104 |F2       |Student      |\n",
      "|352187318272 |GMB      |Business     |\n",
      "|369367187456 |B2       |Pleasure     |\n",
      "|498216206336 |F1       |Student      |\n",
      "|601295421440 |CPL      |Pleasure     |\n",
      "|704374636544 |I1       |Business     |\n",
      "|738734374912 |WB       |Business     |\n",
      "|747324309504 |M1       |Student      |\n",
      "|807453851648 |B1       |Business     |\n",
      "|884763262976 |WT       |Pleasure     |\n",
      "|1151051235328|M2       |Student      |\n",
      "|1314259992576|CP       |Pleasure     |\n",
      "|1331439861760|GMT      |Pleasure     |\n",
      "|1348619730944|E1       |Business     |\n",
      "|1391569403904|I        |Business     |\n",
      "|1554778161152|E2       |Business     |\n",
      "|1709396983808|SBP      |Pleasure     |\n",
      "+-------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_visa = create_dim_visa_type(df)\n",
    "dim_visa.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_visa.createOrReplaceTempView('visa')\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "            SELECT df.*, visa.visa_id\n",
    "            FROM df\n",
    "            JOIN visa ON visa.visa_type = df.visa_type\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------+---------+----------+---------+------------+--------------+------------+\n",
      "|id     |citizenship_code|residence_code|city_code|state_code|visa_type|arrival_date|departure_date|visa_id     |\n",
      "+-------+----------------+--------------+---------+----------+---------+------------+--------------+------------+\n",
      "|1980.0 |104.0           |104.0         |ATL      |MS        |F2       |2016-04-01  |2016-04-08    |103079215104|\n",
      "|51783.0|151.0           |151.0         |NYC      |NY        |F2       |2016-04-01  |2016-04-28    |103079215104|\n",
      "|52934.0|206.0           |206.0         |TOR      |NY        |F2       |2016-04-01  |2016-04-12    |103079215104|\n",
      "+-------+----------------+--------------+---------+----------+---------+------------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('visa_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------+---------+----------+------------+--------------+------------+\n",
      "|id     |citizenship_code|residence_code|city_code|state_code|arrival_date|departure_date|visa_id     |\n",
      "+-------+----------------+--------------+---------+----------+------------+--------------+------------+\n",
      "|1980.0 |104.0           |104.0         |ATL      |MS        |2016-04-01  |2016-04-08    |103079215104|\n",
      "|51783.0|151.0           |151.0         |NYC      |NY        |2016-04-01  |2016-04-28    |103079215104|\n",
      "|52934.0|206.0           |206.0         |TOR      |NY        |2016-04-01  |2016-04-12    |103079215104|\n",
      "+-------+----------------+--------------+---------+----------+------------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo_input_file = 'us-cities-demographics.csv'\n",
    "\n",
    "def create_dim_demographic_table(spark, demo_input_file):\n",
    "    \"\"\"\n",
    "        It spark session and location of a file for demographics data. It creates a demographic table.\n",
    "        \n",
    "        :param: spark: spark session.\n",
    "        :param: demo_input_file: Location of demographic data.\n",
    "        \n",
    "    \"\"\"\n",
    "    df = spark.read.format('csv').option('delimiter', ';').option('header', True).load(demo_input_file, inferSchema=True) \n",
    "    \n",
    "    # Get columns with NA values\n",
    "    columns_to_dropna = []\n",
    "    length = len(df.toPandas().isna().sum())\n",
    "\n",
    "    for i in range(length):\n",
    "        if df.toPandas().isna().sum().values[i] != 0:\n",
    "            columns_to_dropna.append(df.toPandas().isna().sum().index[i])\n",
    "    columns_to_dropna\n",
    "    \n",
    "    # Drop NA values and remove duplicates\n",
    "    df = df.dropna(subset=columns_to_dropna)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['City', 'State', 'State Code', 'Race'])\n",
    "    \n",
    "    df = df.withColumnRenamed('City', 'city')\\\n",
    "    .withColumnRenamed('State', 'state')\\\n",
    "    .withColumnRenamed('Median Age', 'median_age')\\\n",
    "    .withColumnRenamed('Male Population', 'male_population')\\\n",
    "    .withColumnRenamed('Female Population', 'female_population')\\\n",
    "    .withColumnRenamed('Total Population', 'total_population')\\\n",
    "    .withColumnRenamed('Number of Veterans', 'number_of_veterans')\\\n",
    "    .withColumnRenamed('Foreign-born', 'foreign_born')\\\n",
    "    .withColumnRenamed('Average Household Size', 'average_household_size')\\\n",
    "    .withColumnRenamed('State Code', 'state_code')\\\n",
    "    .withColumnRenamed('Race', 'race')\\\n",
    "    .withColumnRenamed('Count', 'count')\n",
    "    \n",
    "    df = df.groupBy('city', 'state', 'median_age', 'male_population', 'female_population', 'total_population', 'number_of_veterans', 'foreign_born', 'average_household_size', 'state_code').pivot('race').agg(F.first(F.col('count')))\n",
    "    \n",
    "    df = df.withColumn('demo_id', F.monotonically_increasing_id())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+-----+-------------------------+------------------+------+-----------+\n",
      "|city        |state       |median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino|White |demo_id    |\n",
      "+------------+------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+-----+-------------------------+------------------+------+-----------+\n",
      "|Modesto     |California  |35.2      |104852         |106405           |211257          |9855              |39613       |2.97                  |CA        |4388                             |19417|9869                     |85141             |166618|0          |\n",
      "|Pittsburgh  |Pennsylvania|32.9      |149690         |154695           |304385          |17728             |28187       |2.13                  |PA        |2689                             |21227|82248                    |9266              |208863|1          |\n",
      "|South Jordan|Utah        |33.8      |33669          |32970            |66639           |2595              |6142        |3.61                  |UT        |null                             |2487 |null                     |4672              |59046 |2          |\n",
      "|Passaic     |New Jersey  |30.4      |36505          |34577            |71082           |693               |24174       |3.56                  |NJ        |null                             |3338 |5230                     |53680             |30352 |8589934592 |\n",
      "|Dale City   |Virginia    |33.4      |35984          |35415            |71399           |6085              |22306       |3.65                  |VA        |548                              |9251 |18074                    |23868             |34640 |8589934593 |\n",
      "|Chino Hills |California  |41.9      |39639          |38674            |78313           |3358              |24764       |3.05                  |CA        |1773                             |28132|3506                     |22539             |40030 |8589934594 |\n",
      "|Ann Arbor   |Michigan    |28.1      |58789          |58281            |117070          |3614              |20717       |2.17                  |MI        |1935                             |18797|9577                     |5888              |90173 |17179869184|\n",
      "|Gilbert     |Arizona     |33.2      |116711         |130812           |247523          |10817             |24531       |3.2                   |AZ        |5965                             |19740|9076                     |39937             |211322|17179869185|\n",
      "|Indianapolis|Indiana     |34.1      |410615         |437808           |848423          |42186             |72456       |2.53                  |IN        |8656                             |29307|253932                   |83426             |553665|17179869186|\n",
      "|Long Beach  |California  |34.6      |238159         |236013           |474172          |17463             |127764      |2.78                  |CA        |12841                            |68095|64948                    |207890            |277962|17179869187|\n",
      "+------------+------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+-----+-------------------------+------------------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographic_table = create_dim_demographic_table(spark, demo_input_file)\n",
    "dim_demographic_table.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the port_codes to the dim_demographic_table.\n",
    "\n",
    "dim_demographic_table.createOrReplaceTempView('demo')\n",
    "port_codes_df.createOrReplaceTempView('ports')\n",
    "\n",
    "dim_demographic_table = spark.sql(\"\"\"\n",
    "                    SELECT demo.*, ports.port_code\n",
    "                    FROM demo\n",
    "                    JOIN ports\n",
    "                    ON lower(ports.city_name) = lower(demo.city) AND ports.state_code = demo.state_code\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+------+-------------------------+------------------+-------+-------------+---------+\n",
      "|city       |state     |median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|American Indian and Alaska Native|Asian |Black or African-American|Hispanic or Latino|White  |demo_id      |port_code|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+------+-------------------------+------------------+-------+-------------+---------+\n",
      "|Anchorage  |Alaska    |32.2      |152945         |145750           |298695          |27492             |33258       |2.77                  |AK        |36339                            |36825 |23107                    |27261             |212696 |1649267441664|ANC      |\n",
      "|Mobile     |Alabama   |38.0      |91275          |103030           |194305          |11939             |7234        |2.4                   |AL        |2816                             |5518  |96397                    |5229              |93755  |146028888065 |MOB      |\n",
      "|Phoenix    |Arizona   |33.8      |786833         |776168           |1563001         |72388             |300702      |2.89                  |AZ        |41748                            |66403 |132939                   |669914            |1161455|249108103171 |PHO      |\n",
      "|Tucson     |Arizona   |33.6      |264893         |266781           |531674          |38182             |82220       |2.45                  |AZ        |24409                            |24689 |33900                    |231025            |404342 |601295421444 |TUC      |\n",
      "|Yuma       |Arizona   |33.4      |48298          |45847            |94145           |7182              |19326       |2.64                  |AZ        |1228                             |1180  |3731                     |57054             |69691  |171798691843 |YUI      |\n",
      "|Fresno     |California|30.0      |256130         |263942           |520072          |18410             |103453      |3.12                  |CA        |11380                            |75318 |46072                    |256145            |325651 |1391569403906|FRE      |\n",
      "|Long Beach |California|34.6      |238159         |236013           |474172          |17463             |127764      |2.78                  |CA        |12841                            |68095 |64948                    |207890            |277962 |17179869187  |LNB      |\n",
      "|Los Angeles|California|35.0      |1958998        |2012898          |3971896         |85417             |1485425     |2.86                  |CA        |63758                            |512999|404868                   |1936732           |2177650|1537598291968|LOS      |\n",
      "|Oakland    |California|35.7      |203827         |215451           |419278          |12159             |113896      |2.56                  |CA        |8380                             |75446 |118228                   |114054            |171599 |206158430209 |OAK      |\n",
      "|Ontario    |California|31.0      |85059          |86141            |171200          |4816              |48557       |3.52                  |CA        |4304                             |14313 |12900                    |118292            |74765  |283467841537 |ONT      |\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+------+-------------------------+------------------+-------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographic_table.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographic_table = dim_demographic_table.withColumnRenamed('American Indian and Alaska Native', 'american_indian_and_alaska_native')\\\n",
    "        .withColumnRenamed('Asian', 'asian')\\\n",
    "        .withColumnRenamed('Black or African-American', 'black_or_african_american')\\\n",
    "        .withColumnRenamed('Hispanic or Latino', 'hispanic_or_latino')\\\n",
    "        .withColumnRenamed('White', 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+-----+-------------------------+------------------+-------+-------------+---------+\n",
      "|city     |state  |median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|american_indian_and_alaska_native|asian|black_or_african_american|hispanic_or_latino|white  |demo_id      |port_code|\n",
      "+---------+-------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+-----+-------------------------+------------------+-------+-------------+---------+\n",
      "|Anchorage|Alaska |32.2      |152945         |145750           |298695          |27492             |33258       |2.77                  |AK        |36339                            |36825|23107                    |27261             |212696 |1649267441664|ANC      |\n",
      "|Mobile   |Alabama|38.0      |91275          |103030           |194305          |11939             |7234        |2.4                   |AL        |2816                             |5518 |96397                    |5229              |93755  |146028888065 |MOB      |\n",
      "|Phoenix  |Arizona|33.8      |786833         |776168           |1563001         |72388             |300702      |2.89                  |AZ        |41748                            |66403|132939                   |669914            |1161455|249108103171 |PHO      |\n",
      "+---------+-------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------------------------------+-----+-------------------------+------------------+-------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographic_table.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographic_table.createOrReplaceTempView('demo')\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "            SELECT df.*, demo_id\n",
    "            FROM demo\n",
    "            JOIN df ON demo.state_code = df.state_code AND demo.port_code = df.city_code\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------+---------+----------+------------+--------------+------------+------------+\n",
      "|id      |citizenship_code|residence_code|city_code|state_code|arrival_date|departure_date|visa_id     |demo_id     |\n",
      "+--------+----------------+--------------+---------+----------+------------+--------------+------------+------------+\n",
      "|287122.0|245.0           |245.0         |BOS      |MA        |2016-04-02  |2016-05-19    |103079215104|884763262977|\n",
      "|494820.0|213.0           |213.0         |BOS      |MA        |2016-04-03  |2016-05-14    |103079215104|884763262977|\n",
      "|581311.0|582.0           |582.0         |BOS      |MA        |2016-04-03  |2016-05-12    |103079215104|884763262977|\n",
      "+--------+----------------+--------------+---------+----------+------------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_input_file = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "def create_dim_temperature_table(spark, temp_input_file):\n",
    "    \"\"\"\n",
    "        It takes spark session and the temperature file location as arguments. It creates a temperature table.\n",
    "        :param: spark: spark session.\n",
    "        :param: temp_input_file: location of temperature data.\n",
    "        \n",
    "    \"\"\"\n",
    "    df = spark.read.format('csv').option('header', True).option('delimiter', ',').load(temp_input_file, inferSchema=True)\n",
    "    \n",
    "    # Get only US data\n",
    "    df = df[df['Country'] == 'United States']\n",
    "    \n",
    "    # Check if there's any NA value. If so, drop them.\n",
    "    df.createOrReplaceTempView('temp')\n",
    "    na_values = spark.sql(\"\"\"\n",
    "                    SELECT AverageTemperature\n",
    "                    FROM temp\n",
    "                    WHERE AverageTemperature IS NULL\n",
    "                \"\"\").count()\n",
    "    print(f'It was found {na_values} NA values.')\n",
    "    df = df.dropna(subset=['AverageTemperature'])\n",
    "    \n",
    "    # Check again\n",
    "    df.createOrReplaceTempView('temp')\n",
    "    na_values = spark.sql(\"\"\"\n",
    "                        SELECT AverageTemperature\n",
    "                        FROM temp\n",
    "                        WHERE AverageTemperature IS NULL\n",
    "                    \"\"\").count()\n",
    "    print(f'It was found {na_values} NA values.')\n",
    "    \n",
    "    # Let's get only date and month\n",
    "    df = df.withColumn('date', F.to_date('dt')).withColumn('month', F.month('dt'))\n",
    "    \n",
    "    # Calculate monthly average temperature for each city.\n",
    "    df.createOrReplaceTempView('temp2')\n",
    "    df = spark.sql(\"\"\"\n",
    "            SELECT month, City, ROUND(AVG(AverageTemperature), 1)\n",
    "            FROM temp2\n",
    "            GROUP BY 1, 2\n",
    "            ORDER BY 1, 2\n",
    "        \"\"\")\n",
    "    df = df.withColumnRenamed('City', 'city')\\\n",
    "        .withColumnRenamed('ROUND(AVG(AverageTemperature), 1)', 'average_temperature')#\\\n",
    "        #.withColumn('id', F.monotonically_increasing_id())\n",
    "    df = df.select('city', 'month', 'average_temperature')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was found 25765 NA values.\n",
      "It was found 0 NA values.\n",
      "+-----------+-----+-------------------+\n",
      "|city       |month|average_temperature|\n",
      "+-----------+-----+-------------------+\n",
      "|Abilene    |1    |5.3                |\n",
      "|Akron      |1    |-3.2               |\n",
      "|Albuquerque|1    |-0.4               |\n",
      "+-----------+-----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature_table = create_dim_temperature_table(spark, temp_input_file)\n",
    "dim_temperature_table.show(3, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_temperature_table = dim_temperature_table.join(port_codes_df, on=F.lower(dim_temperature_table.city)==F.lower(port_codes_df.city_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+---------+----------+\n",
      "|city   |month|average_temperature|port_code|state_code|\n",
      "+-------+-----+-------------------+---------+----------+\n",
      "|Oakland|1    |8.3                |OAK      |CA        |\n",
      "|Oakland|2    |10.3               |OAK      |CA        |\n",
      "|Oakland|3    |11.9               |OAK      |CA        |\n",
      "+-------+-----+-------------------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature_table = dim_temperature_table.drop('location', 'city_name')\n",
    "dim_temperature_table.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-------------------+---------+----------+---------+----------------+--------------+---------+----------+------------+--------------+------------+-------------+\n",
      "|city     |month|average_temperature|port_code|state_code|id       |citizenship_code|residence_code|city_code|state_code|arrival_date|departure_date|visa_id     |demo_id      |\n",
      "+---------+-----+-------------------+---------+----------+---------+----------------+--------------+---------+----------+------------+--------------+------------+-------------+\n",
      "|Rochester|4    |6.4                |ROC      |NY        |1629772.0|245.0           |245.0         |ROC      |NY        |2016-04-09  |2016-06-30    |369367187456|1073741824000|\n",
      "|Rochester|4    |6.4                |ROC      |NY        |3390560.0|509.0           |509.0         |ROC      |NY        |2016-04-18  |2016-04-19    |369367187456|1073741824000|\n",
      "|Rochester|4    |6.4                |ROC      |NY        |5359222.0|582.0           |582.0         |ROC      |NY        |2016-04-28  |2016-05-01    |369367187456|1073741824000|\n",
      "+---------+-----+-------------------+---------+----------+---------+----------------+--------------+---------+----------+------------+--------------+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature_table.join(df, (dim_temperature_table.port_code==df.city_code) & (dim_temperature_table.month==F.month(df.arrival_date)) & (dim_temperature_table.state_code==df.state_code)).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get id field from df.\n",
    "dim_temperature_table = dim_temperature_table.join(df, (dim_temperature_table.port_code==df.city_code) & (dim_temperature_table.month==F.month(df.arrival_date)) & (dim_temperature_table.state_code==df.state_code)).select('id', 'city', 'port_code', dim_temperature_table.state_code, 'month', 'average_temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_temperature_table = dim_temperature_table.withColumnRenamed('id', 'temp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+----------+-----+-------------------+\n",
      "|temp_id  |city     |port_code|state_code|month|average_temperature|\n",
      "+---------+---------+---------+----------+-----+-------------------+\n",
      "|1629772.0|Rochester|ROC      |NY        |4    |6.4                |\n",
      "|3390560.0|Rochester|ROC      |NY        |4    |6.4                |\n",
      "|5359222.0|Rochester|ROC      |NY        |4    |6.4                |\n",
      "+---------+---------+---------+----------+-----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature_table.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels_description = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "def dim_country_table(spark, labels_description):\n",
    "    \"\"\"\n",
    "        It takes spark session and location of SAS labels description. It creates a country table.\n",
    "        \n",
    "        :param: spark: spark session.\n",
    "        :param: labels_description: location for SAS labels description.\n",
    "        \n",
    "    \"\"\"\n",
    "    with open(labels_description, 'r') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "\n",
    "    re_compiled = re.compile(r\"\\s*(\\d{,3})\\s*(=)\\s*\\'(.*)\\'\")\n",
    "    country_codes = {}\n",
    "    for line in data[10:298]:\n",
    "        founds = re_compiled.search(line)\n",
    "    # print(results.group(3))\n",
    "        country_codes[founds.group(1)] = founds.group(3)\n",
    "    countries = list(map(list, country_codes.items()))\n",
    "    country_codes_df = spark.createDataFrame(countries, ['country_code', 'country_name'])\n",
    "    \n",
    "    return country_codes_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_country_table = dim_country_table(spark, labels_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|country_code|country_name|\n",
      "+------------+------------+\n",
      "|236         |AFGHANISTAN |\n",
      "|101         |ALBANIA     |\n",
      "|316         |ALGERIA     |\n",
      "+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_country_table.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------+---------+----------+------------+--------------+------------+------------+\n",
      "|id      |citizenship_code|residence_code|city_code|state_code|arrival_date|departure_date|visa_id     |demo_id     |\n",
      "+--------+----------------+--------------+---------+----------+------------+--------------+------------+------------+\n",
      "|287122.0|245.0           |245.0         |BOS      |MA        |2016-04-02  |2016-05-19    |103079215104|884763262977|\n",
      "|494820.0|213.0           |213.0         |BOS      |MA        |2016-04-03  |2016-05-14    |103079215104|884763262977|\n",
      "|581311.0|582.0           |582.0         |BOS      |MA        |2016-04-03  |2016-05-12    |103079215104|884763262977|\n",
      "+--------+----------------+--------------+---------+----------+------------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------+------------+--------------+------------+------------+\n",
      "|id      |citizenship_code|residence_code|arrival_date|departure_date|visa_id     |demo_id     |\n",
      "+--------+----------------+--------------+------------+--------------+------------+------------+\n",
      "|287122.0|245.0           |245.0         |2016-04-02  |2016-05-19    |103079215104|884763262977|\n",
      "|494820.0|213.0           |213.0         |2016-04-03  |2016-05-14    |103079215104|884763262977|\n",
      "|581311.0|582.0           |582.0         |2016-04-03  |2016-05-12    |103079215104|884763262977|\n",
      "+--------+----------------+--------------+------------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration = df.drop('city_code', 'state_code')\n",
    "fact_immigration.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------------+-----------------+-------------+------+----------------+-------------------+\n",
      "|arrival_month|departure_day|citizenship_country|residence_country|visa_category|gender|total_population|average_temperature|\n",
      "+-------------+-------------+-------------------+-----------------+-------------+------+----------------+-------------------+\n",
      "|4            |3            |AUSTRIA            |AUSTRIA          |Business     |M     |864816          |13.6               |\n",
      "|4            |9            |DENMARK            |DENMARK          |Business     |M     |669469          |6.4                |\n",
      "|4            |6            |FINLAND            |FINLAND          |Pleasure     |F     |8550405         |8.6                |\n",
      "+-------------+-------------+-------------------+-----------------+-------------+------+----------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the data model.\n",
    "dim_country_table.createOrReplaceTempView('country')\n",
    "dim_temperature_table.createOrReplaceTempView('temp')\n",
    "dim_demographic_table.createOrReplaceTempView('demo')\n",
    "dim_visa.createOrReplaceTempView('visa')\n",
    "dim_immigrant.createOrReplaceTempView('immigrant')\n",
    "dim_time_table.createOrReplaceTempView('time')\n",
    "fact_immigration.createOrReplaceTempView('df')\n",
    "\n",
    "#     JOIN visa ON df.visa_id = visa.visa_id\n",
    "#     JOIN temp ON df.id = temp.id\n",
    "#     JOIN immigrant ON df.id = immigrant.id\n",
    "#     JOIN demo ON df.demo_id = demo.demo_id\n",
    "    \n",
    "#     visa.visa_category,\n",
    "#     temp.average_temperature,\n",
    "#     immigrant.gender,\n",
    "#     demo.total_population\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        time.month as arrival_month,\n",
    "        time2.day as departure_day,\n",
    "        country.country_name as citizenship_country,\n",
    "        country2.country_name as residence_country,\n",
    "        visa.visa_category,\n",
    "        immigrant.gender,\n",
    "        demo.total_population,\n",
    "        temp.average_temperature\n",
    "        \n",
    "    FROM df\n",
    "    JOIN time ON df.arrival_date = time.date\n",
    "    JOIN time time2 ON df.departure_date = time2.date\n",
    "    JOIN country ON df.citizenship_code = country.country_code\n",
    "    JOIN country country2 ON df.residence_code = country2.country_code\n",
    "    JOIN visa ON df.visa_id = visa.visa_id\n",
    "    JOIN immigrant ON df.id = immigrant.id\n",
    "    JOIN demo ON df.demo_id = demo.demo_id\n",
    "    JOIN temp ON df.id = temp.temp_id\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save in parquet format. To do so, comment it out and run it.\n",
    "# dim_country_table.write.mode('overwrite').parquet('dim_country')\n",
    "# dim_temperature_table.write.mode('overwrite').partitionBy('city', 'port_code').parquet('dim_temperature')\n",
    "# dim_demographic_table.write.mode('overwrite').partitionBy('city', 'port_code').parquet('dim_demographic')\n",
    "# dim_visa.write.mode('overwrite').parquet('dim_visa')\n",
    "# dim_immigrant.write.mode('overwrite').parquet('dim_immigrant')\n",
    "# dim_time_table.write.mode('overwrite').partitionBy('date').parquet('dim_date')\n",
    "# fact_immigration.write.mode('overwrite').partitionBy('arrival_date').parquet('fact_immigration')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def quality_checks(spark_df, table_name):\n",
    "    \"\"\"\n",
    "        It checks table's completness and if there is any null value.\n",
    "        \n",
    "        :param: spark_df: spark dataframe\n",
    "        :param: table_name: table name\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Completeness quality check has started.')\n",
    "    all_count = spark_df.count()\n",
    "    \n",
    "    if all_count == 0:\n",
    "        print(f'Quality check failed for {table_name}. It does not have data.')\n",
    "    else:\n",
    "        print(f'Quality check passed for {table_name}. It has {all_count} rows.')\n",
    "    \n",
    "    print('NA values quality check has started.')\n",
    "    na_values = spark_df.toPandas().isna().sum()\n",
    "    \n",
    "    if na_values == 0:\n",
    "        print(f'NA quality check passed. There is not NA value in the {table_name}.')\n",
    "    else:\n",
    "        print(f'NA quality check failed. It was found {na_values} in {table_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Description of the data:\n",
    "\n",
    "#### Dimension tables:\n",
    "**country_table**\n",
    "- country_code: integer that represents a country.\n",
    "- country_name: name of a country.\n",
    "\n",
    "**dim_visa**\n",
    "- visa_id: An integer which represents a type of visa.\n",
    "- visa_type: A properly type of visa issued by U.S.\n",
    "- visa_category: A category the visa type belongs to.\n",
    "\n",
    "**dim_date**\n",
    "- date: date based on immigrant's arrival.\n",
    "- year: year extracted from arrival's date.\n",
    "- month: month extracted from arrival's date.\n",
    "- week: week extracted from arrival's date.\n",
    "- day: day extracted from arrival's date.\n",
    "\n",
    "**dim_immigrant**\n",
    "- id: unique value for an immigrant.\n",
    "- age: immigrant's age.\n",
    "- biryear: immigrant's year of birth.\n",
    "- gender: immgrant's gender.\n",
    "\n",
    "**dim_demographic**\n",
    "- demo_id: unique value for demo info.\n",
    "- port_code: port code where an immigrant arrived.\n",
    "- state_code: state code where the port code is located.\n",
    "- city: name of the city for a port code.\n",
    "- median_age: median age for the corresponding city.\n",
    "- male_population: number of male in a city.\n",
    "- female_population: number of female in a city.\n",
    "- total_population: total population for a city.\n",
    "- number_of_veterans: total people considered veteran.\n",
    "- foreign_born: number of people who was born abroad.\n",
    "- avg_household_size: average size of a family living in a house.\n",
    "- american_indian_and_alaska_native: number of indian and alaska native people for a city.\n",
    "- asian: number of asian people for a city.\n",
    "- black_or_african_american: number of black or african american people for a city.\n",
    "- hispanic_or_latino: number of hispanic or latino people for a city.\n",
    "- white: number of white people for a city.\n",
    "\n",
    "**dim_temperature**\n",
    "- id: unique value for temperature info in the table.\n",
    "- city: city name where the average temperature was calculated.\n",
    "- month: reference month when the average temperature was calculated.\n",
    "- port_code: port code where an immigrant arrived.\n",
    "- state_code: state code where the port code is located.\n",
    "- average_temperature: monthly average temperature calculated.\n",
    "\n",
    "##### Fact table\n",
    "**fact_immigration**\n",
    "- id: unique value for an immigration fact.\n",
    "- arrival_date: the date an immigrant arrived in U.S.\n",
    "- departure_date: the date an immigrant left in U.S.\n",
    "- citizenship_code: code for the country an immigrant's citizenship.\n",
    "- residence_code: code for the country an immigrant's residence.\n",
    "- visa_id: unique value for a visa issued by U.S.\n",
    "- demo_id: unique value for a demographic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "For this project the chosen technology was spark mainly because of the following:\n",
    "    - Spark is easy-to-use and the learning curve fot those ones who already know pandas is not steep.\n",
    "    - With spark it is possible to handle many different file formats.\n",
    "    - Spark was built with the goal to work with big data, hence it is totally compatible with cloud technologies such as AWS.\n",
    "    * Propose how often the data should be updated and why.\n",
    "    - Since immigration data is updated monthly, so do our data.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     - Spark was built with the goal to handle big data. In this case, it would be interesting to work with cloud technology such as AWS. In case already working with cloud computing, we may consider increase the number of nodes. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     - For this purpose, we could use Apache Airflow in order to schedule the pipeline.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - In this case, the recommended option would be use a cloud datawarehouse solution such as Amazon Redshift, but taking into consideration the costs involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
